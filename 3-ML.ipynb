{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Julia\n",
    "\n",
    "> Disclaimer: This JupyterNotebook is inspired by the getting started section from MLJ's documentation.\n",
    "\n",
    "One of the packages for Machine Learning in Julia is [MLJ](https://github.com/alan-turing-institute/MLJ.jl).\n",
    "If you are new to this library, which we are(!), MLJ has a good [documentation](https://alan-turing-institute.github.io/MLJ.jl/dev/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to install this package. If you already installed it, you can of course skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/.julia/environments/my_MLJ_env/Project.toml`\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `~/.julia/environments/my_MLJ_env/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `~/.julia/environments/my_MLJ_env/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.activate(\"my_MLJ_env\", shared=true);\n",
    "\n",
    "Pkg.add([\"MLJ\", \"DataFrames\", \"MLJDecisionTreeInterface\", \"Distributions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use/import this library/package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ (Machine Learning in Julia) is a toolbox providing a common interface for selecting, tuing, evaluting, composing, and comparing over 160 ML models. You can take a look at all supported models [here](https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/).\n",
    "\n",
    "You can also take a look at the list of models (model registry) by calling the function `models()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184-element Array{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n",
       " (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n",
       " (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n",
       " (name = ARDRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = AffinityPropagation, package_name = ScikitLearn, ... )\n",
       " (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BaggingRegressor, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " ⋮\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = TSVDTransformer, package_name = TSVD, ... )\n",
       " (name = TfidfTransformer, package_name = MLJText, ... )\n",
       " (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n",
       " (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n",
       " (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateFillImputer, package_name = MLJModels, ... )\n",
       " (name = UnivariateStandardizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )\n",
       " (name = XGBoostCount, package_name = XGBoost, ... )\n",
       " (name = XGBoostRegressor, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that MLJ wraps models from other packages such as ScikitLearn.\n",
    "\n",
    "We will now proceed with the \"Hello World\" example for Machine Learning: The iris dataset.\n",
    "\n",
    "## Obtain Datasest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────┬─────────────┬──────────────┬─────────────┬─────────────────────────────────┐\n",
      "│\u001b[1m sepal_length \u001b[0m│\u001b[1m sepal_width \u001b[0m│\u001b[1m petal_length \u001b[0m│\u001b[1m petal_width \u001b[0m│\u001b[1m target                          \u001b[0m│\n",
      "│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m CategoricalValue{String,UInt32} \u001b[0m│\n",
      "│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Multiclass{3}                   \u001b[0m│\n",
      "├──────────────┼─────────────┼──────────────┼─────────────┼─────────────────────────────────┤\n",
      "│ 5.1          │ 3.5         │ 1.4          │ 0.2         │ setosa                          │\n",
      "│ 4.9          │ 3.0         │ 1.4          │ 0.2         │ setosa                          │\n",
      "│ 4.7          │ 3.2         │ 1.3          │ 0.2         │ setosa                          │\n",
      "└──────────────┴─────────────┴──────────────┴─────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "selectrows(iris, 1:3) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piping operator `|>` allows for function chaining. In the example above, we first select the first three rows and print them pretty.\n",
    "\n",
    "What data type is our `iris` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTuple{(:sepal_length, :sepal_width, :petal_length, :petal_width, :target),Tuple{Array{Float64,1},Array{Float64,1},Array{Float64,1},Array{Float64,1},CategoricalArrays.CategoricalArray{String,1,UInt32,String,CategoricalArrays.CategoricalValue{String,UInt32},Union{}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the schema of `iris` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┬─────────────────────────────────┬───────────────┐\n",
       "│\u001b[22m _.names      \u001b[0m│\u001b[22m _.types                         \u001b[0m│\u001b[22m _.scitypes    \u001b[0m│\n",
       "├──────────────┼─────────────────────────────────┼───────────────┤\n",
       "│ sepal_length │ Float64                         │ Continuous    │\n",
       "│ sepal_width  │ Float64                         │ Continuous    │\n",
       "│ petal_length │ Float64                         │ Continuous    │\n",
       "│ petal_width  │ Float64                         │ Continuous    │\n",
       "│ target       │ CategoricalValue{String,UInt32} │ Multiclass{3} │\n",
       "└──────────────┴─────────────────────────────────┴───────────────┘\n",
       "_.nrows = 150\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "We first split our dataset into training and test. To do so, we can convert this NamedTuple to a [DataFrame](https://dataframes.juliadata.org/stable/) that is similar to Python's `pandas` both in design and functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import DataFrames\n",
    "\n",
    "iris = DataFrames.DataFrame(iris)\n",
    "size(iris)  # get the shape of this DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split features and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────┬─────────────┬──────────────┬─────────────┐\n",
      "│\u001b[1m sepal_length \u001b[0m│\u001b[1m sepal_width \u001b[0m│\u001b[1m petal_length \u001b[0m│\u001b[1m petal_width \u001b[0m│\n",
      "│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64     \u001b[0m│\n",
      "│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous  \u001b[0m│\n",
      "├──────────────┼─────────────┼──────────────┼─────────────┤\n",
      "│ 6.7          │ 3.3         │ 5.7          │ 2.1         │\n",
      "│ 5.7          │ 2.8         │ 4.1          │ 1.3         │\n",
      "│ 7.2          │ 3.0         │ 5.8          │ 1.6         │\n",
      "└──────────────┴─────────────┴──────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "target, features = unpack(iris, ==(:target), colname -> true, rng=123)\n",
    "first(features, 3) |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `first()` is similar to `pandas`' function `head()`, it lets you display or select the first couple of rows (in our example 3). You can also take a look at the last couple of rows by calling `last()`. Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n",
       " \"virginica\"\n",
       " \"versicolor\"\n",
       " \"virginica\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = features[1:100, :]\n",
    "train_target = target[1:100]\n",
    "\n",
    "size(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = features[101:150, :]\n",
    "test_target = target[101:150]\n",
    "size(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model\n",
    "Before we can train a model, we have to select one first. Previously we just took a look at what models are available in general. However, we can also check which models are suitable for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47-element Array{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = BetaML, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " ⋮\n",
       " (name = RandomForestClassifier, package_name = BetaML, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(train_features, train_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, multiple models are suitable. For now, we will stick to a simple model: a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this classifier, we have to load it and bind it to some name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree = @load DecisionTreeClassifier pkg = DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In this case, we need to specify `pkg` because multiple packages provide a model type with the name `DecisionTreeClassifier`.) Now we can instantiate a model with default hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the very first code cell in this JupyterNotebook? It installed some packages like the MLJ library itself, but also `MLJDecisionTreeInterface`. Why did we do that? Because MLJ is just a wrapper and `DecisionTree.jl` is no dependency of MLJ. Therefore, we have to install it ourselves. \n",
    "\n",
    "When you want to use a model (i. e. load and bind it like we did with the decision tree classifier), and this model or package is not in your path, you'll enocunter an error. However, that is no need to panic! The error message will tell you what to do.\n",
    "\n",
    "Ok, so now that we instantiated our decision tree, we can train our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to create a machine\n",
    "mach = machine(tree, train_features, train_target)\n",
    "\n",
    "# And then we can train\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a machine?**\n",
    "\n",
    "A machine binds a model to data and it stores the model's learned parameters. For more information, take a look at the [documentation on machines](https://alan-turing-institute.github.io/MLJ.jl/stable/machines/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Once trained we can evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(mach, test_features)\n",
    "predictions[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(predictions, test_target) |> mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `predictions` contain probabilities of all possible classes. For each element, we now have to fetch the class with the highes probability. We can do that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_predictions = mode.(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot a confusion matrix and calculate the Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(clean_predictions, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(clean_predictions, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you wondering which measures are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fetching the most likely predicted class manually, we can automate it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preditions = predict_mode(mach, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of that (and maybe because we want to manage our recources efficiently), we can train, predict, and evaluate a model of our choice with one single function call: `evaluate()`. This function perofrms all the steps we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tree, train_features, train_target,\n",
    "        resampling=CV(shuffle=true),\n",
    "        measures=[log_loss, accuracy],\n",
    "        verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we do not even need to split the data into training and test, because we can also automate that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach2 = machine(tree, features, target)\n",
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "    measures=[log_loss, accuracy],\n",
    "    verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Play around a little bit and train a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "The getting started guide from MLJ states:\n",
    "\n",
    "\"To learn a little more about what MLJ can do, browse [Common MLJ Workflows](https://alan-turing-institute.github.io/MLJ.jl/dev/common_mlj_workflows/) or [Data Science Tutorials in Julia](https://alan-turing-institute.github.io/DataScienceTutorials.jl/) or try the [JuliaCon2020 Workshop](https://github.com/ablaom/MachineLearningInJulia2020) on MLJ (recorded [here](https://www.youtube.com/watch?time_continue=27&v=qSWbCn170HU&feature=emb_title)) returning to the manual as needed.\"\n",
    "\n",
    "So have fun checking them out if you're interested!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
